{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1 Part 2 - Hayden Barker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-1-45402542f4f0>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch: 1 cost = 26.951  test accuracy: 0.949\n",
      "Epoch: 2 cost = 29.476  test accuracy: 0.968\n",
      "Epoch: 3 cost = 29.959  test accuracy: 0.973\n",
      "Epoch: 4 cost = 29.963  test accuracy: 0.975\n",
      "Epoch: 5 cost = 29.818  test accuracy: 0.979\n",
      "Epoch: 6 cost = 29.954  test accuracy: 0.980\n",
      "Epoch: 7 cost = 30.337  test accuracy: 0.982\n",
      "Epoch: 8 cost = 30.425  test accuracy: 0.984\n",
      "Epoch: 9 cost = 30.202  test accuracy: 0.984\n",
      "Epoch: 10 cost = 29.533  test accuracy: 0.984\n",
      "\n",
      "Training complete!\n",
      "0.9842\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def run_cnn():\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    # Python optimisation variables\n",
    "    learning_rate = 0.001\n",
    "    epochs = 10\n",
    "    batch_size = 100\n",
    "    i = 0\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784 - this is the flattened image data that is drawn from mnist.train.nextbatch()\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    # reshape the input data so that it is a 4D tensor.  The first value (-1) tells function to dynamically shape that\n",
    "    # dimension based on the amount of data passed to it.  The two middle dimensions are set to the image size (i.e. 28\n",
    "    # x 28).  The final dimension is 1 as there is only a single colour channel i.e. grayscale.  If this was RGB, this\n",
    "    # dimension would be 3\n",
    "    x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "   #\"\"\"Model function for CNN.\"\"\"\n",
    "   # Input Layer\n",
    "   # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "   # MNIST images are 28x28 pixels, and have one color channel\n",
    "    input_layer = x_shaped\n",
    "   # Convolutional Layer #1\n",
    "   # Computes 4 features using a 5x5 filter with ReLU activation.\n",
    "   # Padding is added to preserve width and height.\n",
    "   # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "   # Output Tensor Shape: [batch_size, 28, 28, 4]\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer, filters=4, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "   # Pooling Layer #1\n",
    "   # First max pooling layer with a 2x2 filter and stride of 2\n",
    "   # Input Tensor Shape: [batch_size, 28, 28, 4]\n",
    "   # Output Tensor Shape: [batch_size, 14, 14, 4]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "   # Convolutional Layer #2\n",
    "   # Computes 8 features using a 5x5 filter.\n",
    "   # Padding is added to preserve width and height.\n",
    "   # Input Tensor Shape: [batch_size, 14, 14, 4]\n",
    "   # Output Tensor Shape: [batch_size, 14, 14, 8]\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=8, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "   # Pooling Layer #2\n",
    "   # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "   # Input Tensor Shape: [batch_size, 14, 14, 8]\n",
    "   # Output Tensor Shape: [batch_size, 7, 7, 8]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "   # Flatten tensor into a batch of vectors\n",
    "   # Input Tensor Shape: [batch_size, 7, 7, 8]\n",
    "   # Output Tensor Shape: [batch_size, 7 * 7 * 8]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 8])\n",
    "   # Logit layer\n",
    "   # Densely connected layer with 10 neurons\n",
    "   # Input Tensor Shape: [batch_size, 7 * 7 * 8]\n",
    "   # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=pool2_flat, units=10)\n",
    "    y_ = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Create a placeholder for loss to avoid using tensorflow to create it.\n",
    "    cross_entropy = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Use this loss the program will run on the first pass through.\n",
    "    if i == 0:  \n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # Stop using the tensorflow cross entropy\n",
    "    i = 1\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                # create batchs\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                # get the logits and the labesl for the batch\n",
    "                logi, labels = sess.run([logits, y], feed_dict={x: batch_x, y: batch_y})\n",
    "                # calculate the loss outside of tensforflow\n",
    "                loss = calcLoss(logi, labels, 0.1, 0.1)\n",
    "                # feed loss back into optimiser for update\n",
    "                opt = sess.run([optimiser], feed_dict={x: batch_x, y: batch_y, cross_entropy: loss})\n",
    "                avg_cost += loss / total_batch\n",
    "                \n",
    "            test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \" test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "        print(\"\\nTraining complete!\")\n",
    "        print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "def calcLoss(logits, labels, sigma, alpha):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # fix the labels so that they will play nice in numpy\n",
    "    labels = np.expand_dims(labels, axis=0)\n",
    "    labels = np.expand_dims(labels, axis=1)\n",
    "    while len(labels.shape) > 2:\n",
    "        labels = labels[0]\n",
    "        \n",
    "    # get the correct class\n",
    "    labels = np.argmax(labels, 1)\n",
    "    \n",
    "    # fix the logits so that they will play nice in numpy\n",
    "    while len(logits.shape) > 2:\n",
    "        logits = logits[0] \n",
    "\n",
    "    # get the random numbers for ES\n",
    "    e = np.random.normal(size = [1, 10])\n",
    "    e = np.repeat(e, 100, 0)\n",
    "    \n",
    "    # add the jitter to the logits\n",
    "    newLogits = logits + (sigma * e)\n",
    "    \n",
    "    # get predictions based on the logtis with jitter\n",
    "    predictions = np.argmax(newLogits, axis = 1)\n",
    "    acc = np.equal(predictions,labels).astype(int)\n",
    "    acc = np.expand_dims(acc, axis = 1)\n",
    "    acc = np.repeat(acc,10,1)\n",
    "    \n",
    "    # get the update for every prediction that was correct and sum them\n",
    "    goodTheta = np.multiply(acc, e)\n",
    "    \n",
    "    theSum = np.sum(goodTheta, axis=0)\n",
    "    \n",
    "    # add the multipliers to the sum\n",
    "    multiplier = 1 / 100\n",
    "    multiplier = multiplier / sigma\n",
    "    multiplier = alpha * multiplier\n",
    "    multiplier = multiplier * theSum\n",
    "    \n",
    "    # create the update that will be used as theta t+1\n",
    "    update = np.add(logits, multiplier)\n",
    "\n",
    "    # get the euclidean norm of the old logits - the updated ones\n",
    "    loss = loss + np.linalg.norm(np.subtract(logits,update))\n",
    "    #print(loss)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from a good run of the model with the current parameters (there is some fluctuation as expected):\n",
    "    \n",
    "Epoch: 1 cost = 26.556  test accuracy: 0.951\n",
    "Epoch: 2 cost = 29.972  test accuracy: 0.970\n",
    "Epoch: 3 cost = 29.281  test accuracy: 0.977\n",
    "Epoch: 4 cost = 29.999  test accuracy: 0.977\n",
    "Epoch: 5 cost = 30.225  test accuracy: 0.979\n",
    "Epoch: 6 cost = 30.304  test accuracy: 0.983\n",
    "Epoch: 7 cost = 30.156  test accuracy: 0.984\n",
    "Epoch: 8 cost = 30.986  test accuracy: 0.985\n",
    "Epoch: 9 cost = 30.069  test accuracy: 0.985\n",
    "Epoch: 10 cost = 30.268  test accuracy: 0.985\n",
    "\n",
    "Training complete!\n",
    "0.9853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
